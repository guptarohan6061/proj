{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProtonMetric.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZfjRih-sUuO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "d245f3ac-12df-4048-b6b7-e407be7c9682"
      },
      "source": [
        "# Importing relevant libraries\n",
        "%%time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "!pip install dateparser\n",
        "import dateparser\n",
        "!pip install usaddress\n",
        "import usaddress\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import tree\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import pickle\n",
        "!pip install uszipcode\n",
        "from uszipcode import SearchEngine\n",
        "!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "!pip install catboost\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "!pip install pyparsing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.6/dist-packages (0.7.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from dateparser) (2.8.1)\n",
            "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.6/dist-packages (from dateparser) (2019.12.20)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from dateparser) (2018.9)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->dateparser) (1.12.0)\n",
            "Requirement already satisfied: usaddress in /usr/local/lib/python3.6/dist-packages (0.5.10)\n",
            "Requirement already satisfied: future>=0.14 in /usr/local/lib/python3.6/dist-packages (from usaddress) (0.16.0)\n",
            "Requirement already satisfied: probableparsing in /usr/local/lib/python3.6/dist-packages (from usaddress) (0.0.1)\n",
            "Requirement already satisfied: python-crfsuite>=0.7 in /usr/local/lib/python3.6/dist-packages (from usaddress) (0.9.7)\n",
            "Requirement already satisfied: uszipcode in /usr/local/lib/python3.6/dist-packages (0.2.4)\n",
            "Requirement already satisfied: pathlib-mate in /usr/local/lib/python3.6/dist-packages (from uszipcode) (1.0.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from uszipcode) (1.3.17)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from uszipcode) (2.23.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from uszipcode) (19.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pathlib-mate->uszipcode) (1.12.0)\n",
            "Requirement already satisfied: autopep8 in /usr/local/lib/python3.6/dist-packages (from pathlib-mate->uszipcode) (1.5.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->uszipcode) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->uszipcode) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->uszipcode) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->uszipcode) (3.0.4)\n",
            "Requirement already satisfied: pycodestyle>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from autopep8->pathlib-mate->uszipcode) (2.6.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from autopep8->pathlib-mate->uszipcode) (0.10.1)\n",
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.23.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.0.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (2.4.7)\n",
            "CPU times: user 123 ms, sys: 61.5 ms, total: 185 ms\n",
            "Wall time: 20.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXlbFvvZsm9O",
        "colab_type": "code",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5184a863-667c-4d96-9981-ba4653265166"
      },
      "source": [
        "# Getting the dataset from csv or excel file\n",
        "%%time\n",
        "input_data = input(\"Enter the training data location:\")\n",
        "try:\n",
        "   df = pd.read_csv(input_data) \n",
        "except:\n",
        "   df = pd.read_excel(input_data)\n",
        "# Getting the target variable\n",
        "target_variable = input(\"Enter the target variable:\")\n",
        "# Dropping Nan values from target variable\n",
        "df = df.dropna(subset = [target_variable])\n",
        "# Creating another dataframe with just the target variable column \n",
        "df1 = df\n",
        "# Dropping the target variable from original dataset\n",
        "df = df.drop(target_variable, axis=1)\n",
        "# Deciding whether target variable is categorical or continuous\n",
        "cat_or_con = df1[target_variable].nunique()\n",
        "if cat_or_con > 10: # Personal experience parameter\n",
        "   cat_or_con = 'Continuous'\n",
        "else:\n",
        "   cat_or_con = 'Categorical'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the training data location:/content/commentmess.xlsx\n",
            "Enter the target variable:O\n",
            "CPU times: user 165 ms, sys: 12.5 ms, total: 178 ms\n",
            "Wall time: 8.89 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMilzMnofuHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8d829e70-78ca-440d-b7fd-9664fa001e2b"
      },
      "source": [
        "# Getting rid of duplicate rows and columns\n",
        "%%time\n",
        "df = df.drop_duplicates()\n",
        "df = df.loc[:,~df.columns.duplicated()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.54 ms, sys: 55 Âµs, total: 3.59 ms\n",
            "Wall time: 5.65 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gv8oPGrhtD3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "32c58937-1c32-4056-fc6a-b441666583fe"
      },
      "source": [
        "# Removing white spaces\n",
        "%%time\n",
        "df = df.apply(lambda x: x.str.strip() if type(x) == str else x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.98 ms, sys: 0 ns, total: 1.98 ms\n",
            "Wall time: 1.98 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msnRc8aw9oAU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "32faebe2-bdec-483a-f2b0-8dfe2db8844a"
      },
      "source": [
        "# Removing columns with more than 60% missing values\n",
        "%%time\n",
        "df = df.dropna(thresh= 0.6 * len(df), axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.05 ms, sys: 4 Âµs, total: 3.05 ms\n",
            "Wall time: 3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3bgr2MaO4Mj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "508fd90b-e2b9-444e-a413-d8a9f1053bf9"
      },
      "source": [
        "# Modified Date Engineering\n",
        "%%time\n",
        "def dates_engineering(df):\n",
        "    # Selecting all columns with data or time in the name\n",
        "    date_cols = [col for col in df.columns if ('date' in col.lower()) | ('time' in col.lower()) ]\n",
        "    # Date columns are stored as objects. Hence, we need to convert them to datetime. Setting infer datetime format to true ensures\n",
        "    # all different date formats are dealt with. Coercing ensures that any data impurity is converted to NaT.\n",
        "    df[date_cols] = df[date_cols].applymap(lambda x:dateparser.parse(str(x)))\n",
        "    # Removing NaT values(impurities) from data columns\n",
        "    df[date_cols] = df[date_cols].dropna()\n",
        "    # Removing columns with more than 60% missing values\n",
        "    df = df.dropna(thresh= 0.6 * len(df), axis=1)\n",
        "    # Selecting columns of datetime data type\n",
        "    date_columns = list(df.select_dtypes(include=['datetime64[ns]']).columns)\n",
        "    if len(date_columns) == 0: # Date engineering need not be done if there are no date columns\n",
        "       pass\n",
        "    elif len(date_columns) == 1: # Date engineering involves getting timedelta from date, which is done by subtracting from current date\n",
        "       df[date_columns] = (df[date_columns] - np.datetime64('now'))\n",
        "       df[date_columns] = df[date_columns].astype('timedelta64[D]')\n",
        "    else: # If there are more than one date columns, they are reduced to single column by subtraction of dates and converted to timedelta\n",
        "       df[date_columns].reduce(lambda x, y: x - y, l)\n",
        "       df[date_columns] = (df[date_columns] - np.datetime64('now'))\n",
        "       df[date_columns] = df[date_columns].astype('timedelta64[D]')\n",
        "    return df\n",
        "df = dates_engineering(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 672 ms, sys: 49.7 ms, total: 721 ms\n",
            "Wall time: 723 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdPCJ1D_jB2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3acd6e98-4388-4d95-b9cf-52e1e1c463b0"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 14 entries, 0 to 13\n",
            "Data columns (total 1 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   Address  14 non-null     object\n",
            "dtypes: object(1)\n",
            "memory usage: 224.0+ bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRNOnV15E9ef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "94d400b5-11d6-442a-c2dd-8128b640e182"
      },
      "source": [
        "# Date Engineering\n",
        "%%time\n",
        "def dates_engineering(df):\n",
        "    # Selecting all columns with data or time in the name\n",
        "    date_cols = [col for col in df.columns if ('date' in col.lower()) | ('time' in col.lower()) ]\n",
        "    # Date columns are stored as objects. Hence, we need to convert them to datetime. Setting infer datetime format to true ensures\n",
        "    # all different date formats are dealt with. Coercing ensures that any data impurity is converted to NaT.\n",
        "    df[date_cols] = df[date_cols].apply(lambda col: pd.to_datetime(col, infer_datetime_format=True, errors='coerce') \n",
        "              if col.dtypes == object \n",
        "              else col, \n",
        "              axis=0)\n",
        "    # Removing NaT values(impurities) from data columns\n",
        "    df[date_cols] = df[date_cols].dropna()\n",
        "    # Removing columns with more than 60% missing values\n",
        "    df = df.dropna(thresh= 0.6 * len(df), axis=1)\n",
        "    # Selecting columns of datetime data type\n",
        "    date_columns = list(df.select_dtypes(include=['datetime64[ns]']).columns)\n",
        "    if len(date_columns) == 0: # Date engineering need not be done if there are no date columns\n",
        "       pass\n",
        "    elif len(date_columns) == 1: # Date engineering involves getting timedelta from date, which is done by subtracting from current date\n",
        "       df[date_columns] = (df[date_columns] - np.datetime64('now'))\n",
        "       df[date_columns] = df[date_columns].astype('timedelta64[D]')\n",
        "    else: # If there are more than one date columns, they are reduced to single column by subtraction of dates and converted to timedelta\n",
        "       df[date_columns].reduce(lambda x, y: x - y, l)\n",
        "       df[date_columns] = (df[date_columns] - np.datetime64('now'))\n",
        "       df[date_columns] = df[date_columns].astype('timedelta64[D]')\n",
        "    return df\n",
        "df = dates_engineering(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 20.7 ms, sys: 3.94 ms, total: 24.7 ms\n",
            "Wall time: 37.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4tTz9H5YJls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "22984c55-b525-4ca1-c730-7940e1c4afc3"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I like dogs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dogs are luv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cats are amazing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I hate cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Rabbits are cute</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Rabbits are cuter than dogs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Comment\n",
              "0                  I like dogs\n",
              "1                 Dogs are luv\n",
              "2             Cats are amazing\n",
              "3                  I hate cats\n",
              "4             Rabbits are cute\n",
              "5  Rabbits are cuter than dogs"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck70Mo_cljph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature engineering address columns\n",
        "#%%time\n",
        "def address_engineering(df):\n",
        "    # Selecting all columns with address in the name\n",
        "    address_cols = [col for col in df.columns if 'address' in col.lower()]\n",
        "    # Loading dictionary of US zipcodes with states\n",
        "    search = SearchEngine()\n",
        "    if len(address_cols) == 0: # Address engineering need not be done if there are no address columns\n",
        "       pass\n",
        "    elif len(address_cols) == 1: # Using regex to extract zipcode from address column and running it by the database to get state\n",
        "         df['Zipcode'] = df[address_cols].applymap(lambda x: dict(usaddress.parse(str(x))))\n",
        "         #df['Address_State'] = df['Zipcode'].apply(lambda x: zcdb[x].state\n",
        "    else: # Concatenating address columns, using regex to extract zipcode from column and running it by the database to get state\n",
        "         df['Zipcode'] = df[address_cols].apply(lambda x: ' '.join(x), axis = 1)\n",
        "         df['Zipcode'] = df[address_cols].apply(lambda x: usaddress.parse(str(x)).ZipCode)\n",
        "         df['Address_State'] = df['Zipcode'].apply(lambda x: zcdb[x].state)\n",
        "    return df\n",
        "df = address_engineering(df)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mr0lS6mu5cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modified Feature engineering address columns\n",
        "#%%time\n",
        "def address_engineering(df):\n",
        "    # Selecting all columns with address in the name\n",
        "    address_cols = [col for col in df.columns if 'address' in col.lower()]\n",
        "    # Loading dictionary of US zipcodes with states\n",
        "    search = SearchEngine()\n",
        "    if len(address_cols) == 0: # Address engineering need not be done if there are no address columns\n",
        "         pass\n",
        "    elif len(address_cols) == 1: # Using regex to extract zipcode from address column and running it by the database to get state\n",
        "         df['Zipcode'] = df[address_cols].applymap(lambda x: re.findall('[0-9]{4,6}', str(x)))\n",
        "         df['Zipcode'] = df['Zipcode'].apply(lambda x: x[-1] if len(x) != 0 else None)\n",
        "         df['Zipcode'] = df['Zipcode'].apply(lambda col: pd.to_numeric(col, errors='coerce')).astype('Int64')\n",
        "         df['Address_State'] = df['Zipcode'].apply(lambda x: search.by_zipcode(x).state)     \n",
        "    else: # Concatenating address columns, using regex to extract zipcode from column and running it by the database to get state\n",
        "         df['Zipcode'] = df[address_cols].apply(lambda x: ' '.join(x), axis = 1)\n",
        "         df['Zipcode'] = df[address_cols].applymap(lambda x: re.findall('[0-9]{4,6}', str(x)))\n",
        "         df['Zipcode'] = df['Zipcode'].apply(lambda x: x[-1] if len(x) != 0 else None)\n",
        "         df['Zipcode'] = df['Zipcode'].apply(lambda col: pd.to_numeric(col, errors='coerce')).astype('Int64')\n",
        "         df['Address_State'] = df['Zipcode'].apply(lambda x: search.by_zipcode(x).state)\n",
        "    return df\n",
        "df = address_engineering(df)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4BkiySi4Uhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Numeric Engineering\n",
        "%%time\n",
        "def numeric_engineering(df):\n",
        "  # Getting categorical columns that should be numeric\n",
        "  df = df.apply(lambda col: pd.to_numeric(col, errors='ignore') \n",
        "              if col.dtypes == object \n",
        "              else col, \n",
        "              axis=0)\n",
        "  # Defining a dictionary mapping k, million etc. to numeric meaning\n",
        "  num_map = {'k':1000, 'm':1000000, 'b':1000000000, 'thousand':1000, 'million':1000000, 'billion':1000000000}\n",
        "  object_columns = list(df.select_dtypes(include=['object']).columns)\n",
        "  for obj in object_columns: # Going thorugh all object columns\n",
        "    if df[obj].str.contains('^[1-9][0-9]{1,}\\s{0,}[k,m,b,million,billion,thousand]$', regex=True).any(axis=0) == True : \n",
        "       # If column has numbers followed by k ,million etc.\n",
        "       df[obj] = df[obj].map(lambda x: (float(x[:-1]) * num_map.get(x[-1].lower(), 1))) # Replacing dictionary keys with values\n",
        "       df[obj] = df[obj].astype('float64')\n",
        "    # Getting numbers from gibberish\n",
        "    if df[obj].str.contains('\\d{1,}.?\\d{1,}', regex=True).any(axis=0) == True:\n",
        "       df[obj] = df[obj].str.extract(pat = '(\\d{1,}\\.?\\d{1,})') \n",
        "       df[obj] = df[obj].astype('float64')\n",
        " # As far as numbers with commas go, I see no harm in applying the same over the whole dataframe\n",
        "  try:\n",
        "    df[object_columns].apply(lambda x: x.str.replace(',', '').astype(float), axis=1)\n",
        "  except:\n",
        "    pass\n",
        "  return df\n",
        "df = numeric_engineering(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O50fPRStDRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b0d0216c-015f-4656-fbae-6cc87b295814"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 6 entries, 0 to 5\n",
            "Data columns (total 1 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   Comment  6 non-null      object\n",
            "dtypes: object(1)\n",
            "memory usage: 96.0+ bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLbWFdDUNfbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ce7d312d-e496-4dcf-edce-70aa767adc30"
      },
      "source": [
        "# Dealing with comments\n",
        "#%%time\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "def sentiment_classifier(x): # Ensuring continuous polarity is grouped into three classes of positive, neutral and negative\n",
        "               if x > 0.1:\n",
        "                  x = 1\n",
        "               elif x < -0.1:\n",
        "                  x = -1\n",
        "               else:\n",
        "                  x = 0\n",
        "               return float(x)\n",
        "def sentiment_calc(text):\n",
        "    try:\n",
        "        return TextBlob(text).sentiment.polarity\n",
        "    except:\n",
        "        return None\n",
        "def NLP(df):\n",
        "    comment_cols = [col for col in df.select_dtypes(include=['object']).columns if ('address' not in col.lower())]\n",
        "    if len(comment_cols) > 0:\n",
        "       for col in comment_cols: \n",
        "         #if df[col].nunique() > (0.5 * len(df)):\n",
        "           #if df[col].str.contains('\\d{1,}.?\\d{1,}', regex=True).any(axis=0) == True:\n",
        "              # Lowercasing and spellchecking\n",
        "              df = df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
        "              spell = Speller()\n",
        "              df = df.applymap(lambda s:spell(s) if type(s) == str else s)\n",
        "              # Punctuation removal\n",
        "              df[col] = df[col].str.replace('[^\\w\\s]','')\n",
        "              # Stopword removal\n",
        "              stop = stopwords.words('english')\n",
        "              df[col] = df[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "              # Getting polarity\n",
        "              Polarity = str(col) + 'Polarity'\n",
        "              df[Polarity] = df[col].apply(sentiment_calc)\n",
        "              df[Polarity] = df[Polarity].apply(lambda x: sentiment_classifier(x)) # Defining sentiment on basis of polarity\n",
        "              # Lemmatization\n",
        "              df[col] = df[col].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "              # Tokenization\n",
        "              df[col] = df[col].apply(lambda x: TextBlob(x).words)\n",
        "    return df, comment_cols\n",
        "df, comment_cols = NLP(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_ClQITw6awt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "1b358bfa-958b-48a8-e6f5-47dba10f0f6d"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment</th>\n",
              "      <th>CommentPolarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[like, dog]</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[dog, lui]</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[cat, amazing]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[hate, cat]</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[rabbit, cute]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[rabbit, cuter, dog]</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Comment  CommentPolarity\n",
              "0           [like, dog]              0.0\n",
              "1            [dog, lui]              0.0\n",
              "2        [cat, amazing]              1.0\n",
              "3           [hate, cat]             -1.0\n",
              "4        [rabbit, cute]              1.0\n",
              "5  [rabbit, cuter, dog]              0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E4uxSZfo7bR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "a42aa013-3c1e-443b-bc29-b5e52c8e921a"
      },
      "source": [
        "if len(comment_cols) > 0:\n",
        "   for col in comment_cols: \n",
        "       #if df[col].nunique() > (0.5 * len(df)):\n",
        "           # Creating document word matrix and saving the model\n",
        "              cv = CountVectorizer(analyzer='word',       \n",
        "                             min_df=3,\n",
        "                             lowercase=True,                   \n",
        "                             token_pattern='[a-zA-Z0-9]{3,}')\n",
        "              df[col] = df[col].apply(lambda x: \" \".join(x))\n",
        "              cv.fit(df[col])\n",
        "              cvname = 'cv_model.sav'\n",
        "              pickle.dump(cv, open(cvname, 'wb'))\n",
        "              X_LDA = cv.transform(df[col])\n",
        "              # Applying LDA and saving the model\n",
        "              from sklearn.decomposition import LatentDirichletAllocation\n",
        "              lda_model = LatentDirichletAllocation(n_components=20,               \n",
        "                                      learning_method='online',   \n",
        "                                      random_state=100,          \n",
        "                                      batch_size=128,                 \n",
        "                                      n_jobs = -1)\n",
        "              lda_model.fit(X_LDA)\n",
        "              ldaname = 'lda_model.sav'\n",
        "              pickle.dump(lda_model, open(ldaname, 'wb'))\n",
        "              lda_output = lda_model.transform(X_LDA)\n",
        "              topic_extraction = np.apply_along_axis(lda_output.index(max(lda_output)), 1, lda_output)\n",
        "              # Getting topic\n",
        "              df[col] = pd.DataFrame(topic_extraction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-28b31341f9a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                              token_pattern='[a-zA-Z0-9]{3,}')\n\u001b[1;32m      9\u001b[0m               \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m               \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m               \u001b[0mcvname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cv_model.sav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \"\"\"\n\u001b[1;32m   1185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1148\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1151\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uZAbRJ8PAx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing object columns with more than 60 levels\n",
        "%%time\n",
        "ini_objects = list(df.select_dtypes(include=['object']))\n",
        "for x in ini_objects:\n",
        "    if df[x].nunique() > 60:\n",
        "       df = df.drop(x, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcPSe1AJch9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Identifying continuous and categorical variable columns\n",
        "%%time\n",
        "def ident(df):\n",
        " datatypes = [] # List where categorical or continuous info will be stored\n",
        " for var in df.columns:\n",
        "    catorcon = df[var].nunique()\n",
        "    if catorcon > 10: # Personal experience parameter\n",
        "       catorcon = 1 # Continuous\n",
        "    else:\n",
        "       catorcon = 0 # Categorical\n",
        "    datatypes.append(catorcon)\n",
        "# Ensuring categorical columns are object dtype\n",
        " for i in range(len(datatypes)):\n",
        "  if datatypes[i] == 0:\n",
        "     df.iloc[:,i] = df.iloc[:,i].astype('category')\n",
        " return df,datatypes\n",
        "df,datatypes = ident(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTwU1VcYzLtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lowercasing and spell checking all strings\n",
        "%%time\n",
        "df = df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
        "spell = Speller()\n",
        "df = df.applymap(lambda s:spell(s) if type(s) == str else s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnVARVQhJsCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grouping categorical classes with less than 0.5% values into a singular class 'combined'\n",
        "%%time\n",
        "for i in datatypes:\n",
        "  if i == 0:\n",
        "     frequencies = df.iloc[:,i].value_counts(normalize=True)\n",
        "     comfreq = frequencies[frequencies<0.005]\n",
        "     df.iloc[:,i].replace(comfreq.index,'combined')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouOU0bemzvyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing columns with Pearson Correlation > 0.85\n",
        "%%time\n",
        "def correlation(dataset, threshold):\n",
        "    col_corr = set() # Set of all the names of deleted columns\n",
        "    corr_matrix = dataset.corr() # Correlation matrix\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr): # Elements above threshold\n",
        "                colname = corr_matrix.columns[i] # Getting the name of column\n",
        "                col_corr.add(colname) # To avoid removal of more columns than necessary, storing the column above threshold value \n",
        "                if colname in dataset.columns:\n",
        "                    del dataset[colname] # Deleting the column from the dataset\n",
        "    return dataset\n",
        "df = correlation(df,0.85)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0xhNy9SFMLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Categorical imputation\n",
        "%%time\n",
        "objects = list(df.select_dtypes(include=['object']))\n",
        "df[objects] = df[objects].fillna(value='missing') # Filling Nan values with missing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoP56A7FFz_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting data into train and test\n",
        "%%time\n",
        "X = df\n",
        "post_smote = X.columns\n",
        "y = df1[target_variable]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Performing 80/20 split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQw_J0rs8Swu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Power transform on numerical variables\n",
        "%%time\n",
        "fl_and_int = list(X_train.select_dtypes(include=['float','int'])) # Selecting numeric columns\n",
        "a = 0\n",
        "pt = PowerTransformer(method='yeo-johnson') # Using yeo johnson, since we can have both positive and negative values\n",
        "if len(fl_and_int) > 0:\n",
        " pt.fit(X_train[fl_and_int]+.0001) # Adding this to ensure there are no zero values\n",
        " X_train[fl_and_int] = pt.transform(X_train[fl_and_int]+.0001)\n",
        " X_test[fl_and_int] = pt.transform(X_test[fl_and_int]+.0001) \n",
        " a = 1\n",
        " ptname = 'pt_model.sav' # Saving the model\n",
        " pickle.dump(pt, open(ptname, 'wb'))\n",
        "if cat_or_con == 'Continuous': # The target varibale needs to be transformed too if it is continuous\n",
        "  pt1 = PowerTransformer(method='yeo-johnson') \n",
        "  pt1.fit(y_train.to_numpy().reshape(-1, 1) + .0001)\n",
        "  y_train = pt1.transform(y_train.to_numpy().reshape(-1, 1) + .0001)\n",
        "  y_test = pt1.transform(y_test.to_numpy().reshape(-1, 1) + .0001) \n",
        "  pt1name = 'pt1_model.sav' # Saving the model\n",
        "  pickle.dump(pt1, open(pt1name, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0rFroMBAUvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Outlier removal\n",
        "%%time\n",
        "if len(fl_and_int) > 0: # Replacing values greater than 4 or lesser than -4 with Nan\n",
        " X_train[fl_and_int].where((np.abs(X_train[fl_and_int] > 4)) ,np.NaN, inplace=True)\n",
        " X_test[fl_and_int].where((np.abs(X_train[fl_and_int] > 4)) ,np.NaN, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Mh9H1Fik6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Continuous Imputation, filling Nan values with median\n",
        "%%time\n",
        "X_train[fl_and_int] = X_train[fl_and_int].fillna(X_train[fl_and_int].median()) \n",
        "X_test[fl_and_int] = X_test[fl_and_int].fillna(X_test[fl_and_int].median()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3lhTxasdBw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Oversampling/Undersampling\n",
        "%%time\n",
        "if cat_or_con == 'Categorical':\n",
        "   y_train = pd.Series(y_train)\n",
        "   n = y_train.nunique() # Getting number of unique values\n",
        "   class_percentage = y_train.value_counts(normalize=True) * 100 # Getting distribution of values\n",
        "   target_level = class_percentage.min() # Finding class with least value count\n",
        "   if target_level > (1/(50*n)):\n",
        "      sm = RandomOverSampler(random_state=42)\n",
        "      X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "   else:\n",
        "      sm = RandomOverSampler(random_state=42)\n",
        "      X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "      class_percentage = y_train.value_counts(normalize=True) * 100\n",
        "      target_level = class_percentage.min()\n",
        "      if target_level > (1/(50*n)):\n",
        "         sm = RandomOverSampler(random_state=42)\n",
        "         X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "      else:\n",
        "         um = RandomUnderSampler(random_state=42)\n",
        "         X_train, y_train = um.fit_resample(X_train, y_train)\n",
        "   X_train = pd.DataFrame(X_train, columns = post_smote)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE50I5BvUtTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After SMOTE, all columns become objects, which we don't want, so the numeric ones are converted back to numeric\n",
        "X_train = X_train.apply(lambda col: pd.to_numeric(col, errors='ignore') \n",
        "              if col.dtypes == object \n",
        "              else col, axis=0)\n",
        "y_train = pd.to_numeric(y_train.flatten(), errors='ignore')\n",
        "y_train = pd.DataFrame(y_train, columns = [target_variable])\n",
        "y_test = pd.DataFrame(y_test, columns = [target_variable])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PpjxTT3C4n9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting categorical columns to encoded ones via one hot encoding\n",
        "%%time\n",
        "categorical_cols = list(X_train.select_dtypes(include=['object']))\n",
        "print(categorical_cols)\n",
        "if len(categorical_cols) > 0:\n",
        "   X_train = pd.concat([X_train.drop(categorical_cols, axis=1), pd.get_dummies(X_train[categorical_cols])], axis=1)\n",
        "   X_test = pd.concat([X_test.drop(categorical_cols, axis=1), pd.get_dummies(X_test[categorical_cols])], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1WwK3xnchOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "miss_cols = X_train.columns.difference(X_test.columns) #columns in X_test that are not in X_train\n",
        "if len(miss_cols) > 0: # This only needs to be applied if there is actually a disparity between the train and test columns\n",
        "   for col in miss_cols:\n",
        "    try:\n",
        "       m = int(X_train[col].mean()) # In case of missing columns in test, the same is generated with mean values of column in train\n",
        "       X_test[col] = m\n",
        "    except:\n",
        "       m = int(X_test[col].mean()) # In case of missing columns in train, the same is generated with mean values of column in test\n",
        "       X_train[col] = m\n",
        "cols = X_train.columns.tolist()\n",
        "X_test = X_test[cols] # Ensuring train and test have same columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKrx5ndxDYWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature selection\n",
        "%%time\n",
        "if cat_or_con == 'Categorical':\n",
        " model = DecisionTreeClassifier(max_depth = 50, min_samples_leaf = 5) \n",
        "else: \n",
        " model = DecisionTreeRegressor(max_depth = 50, min_samples_leaf = 5)\n",
        "model.fit(X_train, y_train)\n",
        "feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
        "feat_importances.nlargest(5).plot(kind='barh') # Plotting top 5 important features on horizontal bar graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRvMFkT-uimN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input for model selection, since model selection does not require us to try different models over whole dataset\n",
        "%%time\n",
        "if len(X_train) <= 10000:\n",
        "  input_X_train = X_train\n",
        "  input_y_train = y_train\n",
        "elif len(X_train) > 10000 & len(X_train) <= 100000:\n",
        "  input_X_train = X_train.sample(frac=0.8, random_state=1)\n",
        "  input_y_train = y_train.sample(frac=0.8, random_state=1)\n",
        "elif len(X_train) > 100000 & len(X_train) <= 1000000:\n",
        "  input_X_train = X_train.sample(frac=0.7, random_state=1)\n",
        "  input_y_train = y_train.sample(frac=0.7, random_state=1)\n",
        "else:\n",
        "  input_X_train = X_train.sample(frac=0.5, random_state=1)\n",
        "  input_y_train = y_train.sample(frac=0.5, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov4aTkq6BN7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "if cat_or_con == 'Continuous':\n",
        "     import Regression\n",
        "     name,mod,acc,par = Regression.best_model_reg(input_X_train.values, X_test.values, input_y_train.values.ravel(), y_test.values.ravel())\n",
        "else:\n",
        "     import classification\n",
        "     name,mod,acc,par = classification.best_model_class(input_X_train.values, X_test.values, input_y_train.values.ravel(), y_test.values.ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpWwPkhhYK4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqIRcSPxUNv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting best model over whole dataset\n",
        "mod.fit(X_train,y_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBBzDEgyv5Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the model\n",
        "filename = 'finalized_model.sav' \n",
        "pickle.dump(mod, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPjhLlf1Ilqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scoring test data\n",
        "%%time\n",
        "df1 = pd.read_csv(input(\"Enter the test data location:\"))\n",
        "y1 = df1[target_variable]\n",
        "df1 = df1.loc[:,~df1.columns.duplicated()]\n",
        "df1 = df1.apply(lambda x: x.str.strip() if type(x) == str else x)\n",
        "df1 = dates_engineering(df1)\n",
        "df1 = address_engineering(df1)\n",
        "df1 = numeric_engineering(df1)\n",
        "df1, comment_cols = NLP(df1)\n",
        "if len(comment_cols) > 0:\n",
        "   for col in comment_cols: \n",
        "       if df1[col].nunique() > (0.5 * len(df1)):\n",
        "              cv = pickle.load(open(cvname, 'rb'))\n",
        "              X_LDA = cv.transform(df1[col])\n",
        "              lda = pickle.load(open(ldaname, 'rb'))\n",
        "              lda_output = lda_model.transform(X_LDA)\n",
        "              topic_extraction = np.apply_along_axis(lda_output.index(max(lda_output)), 1, lda_output)\n",
        "              df1[col] = pd.DataFrame(topic_extraction)\n",
        "ini_objects = list(df1.select_dtypes(include=['object']))\n",
        "for x in ini_objects:\n",
        "    if df1[x].nunique() > 60:\n",
        "       df1 = df1.drop(x, axis=1)\n",
        "df1,datatypes = ident(df1)\n",
        "df1 = df1.applymap(lambda s:s.lower() if type(s) == str else s)\n",
        "#spell = Speller()\n",
        "#df1 = df1.applymap(lambda s:spell(s) if type(s) == str else s)\n",
        "for i in range(len(datatypes)):\n",
        "  if datatypes[i] == 0:\n",
        "     df1.iloc[:,i] = df1.iloc[:,i].astype('category')\n",
        "     frequencies = df1.iloc[:,i].value_counts(normalize=True)\n",
        "     comfreq = frequencies[frequencies<0.005]\n",
        "     df1.iloc[:,i].replace(comfreq.index,'combined')\n",
        "df1[objects] = df1[objects].fillna(value='missing')\n",
        "df1 = correlation(df1,0.9)\n",
        "X1 = df1.drop(target_variable, axis=1)\n",
        "if a==1:\n",
        " pt = pickle.load(open(ptname, 'rb'))\n",
        " X1[fl_and_int] = pt.transform(X1[fl_and_int])\n",
        "if cat_or_con == 'Continuous':\n",
        " pt1 = pickle.load(open(pt1name, 'rb'))\n",
        " y1 = pt1.transform(y1.to_numpy().reshape(-1, 1))\n",
        "X1[fl_and_int] = X1[fl_and_int].fillna(X1[fl_and_int].median()) \n",
        "if len(categorical_cols)>0:\n",
        " X1 = pd.concat([X1.drop(categorical_cols, axis=1), pd.get_dummies(X1[categorical_cols])], axis=1)\n",
        "miss_cols = X_train.columns.difference(X1.columns) \n",
        "if len(miss_cols) > 0:\n",
        "   for col in miss_cols:\n",
        "    try:\n",
        "       m = int(X_train[col].mean())\n",
        "       X1[col] = m\n",
        "    except:\n",
        "       X1 = X1.drop(col, axis=1)\n",
        "cols = X_train.columns.tolist()\n",
        "X1 = X1[cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7K4g1Yvqv-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making predictions on scoring dataset\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "abc = loaded_model.predict(X1)\n",
        "abc = pt1.inverse_transform(abc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L6iYZoFhOSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scoring\n",
        "if cat_or_con == 'Continuous':\n",
        "   from sklearn.metrics import r2_score\n",
        "   X1.fillna(X1.median(),inplace=True)\n",
        "   score = r2_score(y1, abc)\n",
        "else:\n",
        "   from sklearn.metrics import confusion_matrix\n",
        "   X1.fillna(X1.median(),inplace=True)\n",
        "   score = confusion_matrix(y1, abc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}